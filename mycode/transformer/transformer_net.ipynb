{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Because the network eliminates convolutional layers, and recurrent layers it must have a way to understand the position\n",
    "of words in respect to one another. Saying 'I saw a movie.' is not the same as 'Movie a I saw.' In order to do this\n",
    "sine and cosine is used to correspond each dimension of the positional encoding to a sinusoid wave. Note dimensions of\n",
    "encodings share same dimensions as d model so they can be summed together. (i.e. d model = embedding dimensions). Also,\n",
    "some inputs are dropped to reduce overfitting when passing inputs from the multi-head attention sub-layer.\n",
    "\"\"\"\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, dropout, maxlen):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Positional Encoding Sinusoid Formula\n",
    "        pos_encoding = torch.zeros(maxlen, d_model)\n",
    "        pos = torch.arange(0, maxlen, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "        denom = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0)) / d_model) # 1000^(2i/d_model)        \n",
    "        pos_encoding[:, 0::2] = torch.sin(pos * denom)\n",
    "        pos_encoding[:, 1::2] = torch.cos(pos * denom)\n",
    "        \n",
    "        # saving encoingss without gradients\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pos_encoding', pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding):\n",
    "        # Residual connection + positional encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transformer network architecture declaration. The basic architecture requires the number of tokens (i.e. total amount of unique words),\n",
    "d model described as the dimensions the encoder/decoder expects as inputs (embedding projections), number of heads which identifies \n",
    "the number of heads used for the multi-head attention model (i.e. number of linear projections for Q, K, & V), number of encoder layers\n",
    "which decides the amount of multi-head attention (normed) & feed forward (normed) sequences, number of decoder layers which entails the \n",
    "total amount of masked multi-head attention (normed), multi-head attention (normed), & feed forward (norm) sequences, feed forward which describes\n",
    "the amount of hidden dimensions to in the feed forward network, and lastly dropout which indicates the percentage of inputs to be dropped\n",
    "when passing to another sub-layer (i.e. percent of inputs to be dropped in masked mult-head attention, multi-head attention, & feed forward\n",
    "netowrk).\n",
    "\n",
    "\n",
    "Encoder\n",
    "Inputs are fed into the embedding layer of the encoder where the tokenized inputs are transformed into vectors of values with the dimensions\n",
    "equivalent to d model. Next the embeddings are positionaly encoded as described in the PositionalEncoding class above. Results from this\n",
    "layer are dropped (based on dropout percentage) and passed to the 1st & 2nd sub-layer of the encoder through residual connections.\n",
    "\n",
    "The multi-head attention layer (1st sub-layer) recieves the positional encodings and computes the weighted similarity of values based on\n",
    "the compatability function between the queries & keys for each value (i.e. weighted a word based on similarities with other words). These\n",
    "are concatenated into h (h being the number of heads) learnable linear projections which are then normalized.\n",
    "\n",
    "The feed forward network (2nd sub-layer) recieves these normalized projections and processes it such that it fits better\n",
    "to the next multi-head attention layer. This result is normalized and passed to the unmasked multi-head attention layer in\n",
    "the decoder.\n",
    "\n",
    "Decoder\n",
    "The decoder takes the output (input from encoder shifted right) embeds it, then positionaly encodes it (described above). \n",
    "As mentioned the result of the positional encodings are dropped and passed to the 1st, 2nd, & 3rd sub-layer through\n",
    "residual connections. \n",
    "\n",
    "The masked multi-head attention layer (1st sub-layer) recieves the positional encodings and operates the same as the normal multi-head attention\n",
    "layer (described above), but it masks positions to prevent positions from attenting to subsequent positions (i.e. it makes sure a position can \n",
    "only be predicted from the positions that preceed it). The result is normalized then passed to the unmasked multi-head attention\n",
    "layer. \n",
    "\n",
    "The unmasked mutli-head attention (2nd sub-layer) layer recieves the result from the 1st sub-layer, but also the normalized result from the encoders\n",
    "feed forward network. It then peforms the same computation as the multi-head attention layer described in the encoder. This result is \n",
    "normalized then fed into the feed forward network.\n",
    "\n",
    "The feed forward network (3rd sub-layer) peforms the same operations as described in the encoder. The result is then normalzed and passed to \n",
    "a linear function so that softmax can be applied to compute the probabilities for the next predicted word.\n",
    "\n",
    "Note: residual connections are used to prevent the vanishing gradient issue as gradients are back-propgated when learning\n",
    "\n",
    "Defaults\n",
    "d_model:\n",
    "model defaults to 512 dimensions (less dimensions means less information passed between sub-layers with a gain of computation, \n",
    "while more means more information at a cost of compututation)\n",
    "\n",
    "n_head:\n",
    "defaults to 8 (scaled dot-product attention layers is proportional to number of heads. i.e. the amount of \n",
    "learnable linear projections is dependent on the number of heads)\n",
    "\n",
    "n_encoder:\n",
    "defaults to 6 and defines the amount of encoder stacks to be used in the transformer network\n",
    "\n",
    "n_decoder:\n",
    "defaults to 6 and defines the amount of decoder stacks to be used in the transformer network\n",
    "\n",
    "feed_forward:\n",
    "defaults to 2048 hidden dimensions (increasing dimensions may help improve fit of input to next multi-head attention layer)\n",
    "\n",
    "dropout: \n",
    "defaults to 0.1 or 10%. Dropout helps prevent overfitting, but dropping too many inputs may prevent the network from learning\n",
    "anything, while dropping not enough may cause the network to overfit\n",
    "\"\"\"\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, n_tokens: int, d_model: int=512, n_head: int=8, n_encoder: int=6, n_decoder: int=6, feed_forward: int=2048, dropout: float=0.1):         \n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # transformer layers\n",
    "        self.embedding = nn.Embedding(n_tokens, d_model)\n",
    "        self.pos_encoder = PositionalEncoder(d_model=d_model, dropout=dropout, maxlen=5000)\n",
    "        self.transformer = nn.Transformer(d_model=d_model, nhead=n_head, num_encoder_layers=n_encoder, \n",
    "                                        num_decoder_layers=n_decoder, dim_feedforward=feed_forward, dropout=dropout)\n",
    "        self.out = nn.Linear(d_model, n_tokens)\n",
    "        \n",
    "    def forward(self, src: torch.LongTensor, tgt: torch.LongTensor, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None) -> torch.LongTensor:\n",
    "        # src shape: (batch_size, src seq_length), tgt shape: (batch_size, tgt seq_length)\n",
    "\n",
    "        # embedding + positional encoding shape: (batch_size, sequence length, d_model)\n",
    "        src = self.embedding(src) * np.sqrt(self.d_model)\n",
    "        tgt = self.embedding(tgt) * np.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        \n",
    "        # reshape: (sequence length, batch_size, d_model),\n",
    "        src = src.permute(1,0,2)\n",
    "        tgt = tgt.permute(1,0,2)\n",
    "\n",
    "        # transformer inputs shape: (sequence length, batch_size, n_tokens)\n",
    "        decoder_out = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=tgt_pad_mask)\n",
    "\n",
    "        # compute linear function and apply softmax (Cross Entropy Loss already does this just return n_tokens as out)\n",
    "        out = self.out(decoder_out)\n",
    "        return out\n",
    "    \n",
    "    # create mask for tgt\n",
    "    def get_tgt_mask(self, seq_length: int) -> torch.LongTensor:\n",
    "        # keep decoder from peeking ahead (i.e. show one word at a time in the sequence)\n",
    "        mask = torch.tril(torch.ones(seq_length, seq_length) == 1) # Lower triangular matrix\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "        return mask\n",
    "    \n",
    "    # create binary encoded matrix ignore pad\n",
    "    def create_pad_mask(self, matrix: torch.LongTensor, pad_val: int) -> torch.BoolTensor:\n",
    "        # matrix = [3, 2, 1, 8, 0, 0, 0], pad_v = 0 -> [False, False, False, False, True, True, True]\n",
    "        return (matrix == pad_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_data(n):\n",
    "    SOS_token = np.array([2])\n",
    "    EOS_token = np.array([3])\n",
    "    length = 8\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # 1,1,1,1,1,1 -> 1,1,1,1,1\n",
    "    for i in range(n // 3):\n",
    "        X = np.concatenate((SOS_token, np.ones(length), EOS_token))\n",
    "        y = np.concatenate((SOS_token, np.ones(length), EOS_token))\n",
    "        data.append([X, y])\n",
    "\n",
    "    # 0,0,0,0 -> 0,0,0,0\n",
    "    for i in range(n // 3):\n",
    "        X = np.concatenate((SOS_token, np.zeros(length), EOS_token))\n",
    "        y = np.concatenate((SOS_token, np.zeros(length), EOS_token))\n",
    "        data.append([X, y])\n",
    "\n",
    "    # 1,0,1,0 -> 1,0,1,0,1\n",
    "    for i in range(n // 3):\n",
    "        X = np.zeros(length)\n",
    "        start = np.random.randint(0, 1)\n",
    "\n",
    "        X[start::2] = 1\n",
    "\n",
    "        y = np.zeros(length)\n",
    "        if X[-1] == 0:\n",
    "            y[::2] = 1\n",
    "        else:\n",
    "            y[1::2] = 1\n",
    "\n",
    "        X = np.concatenate((SOS_token, X, EOS_token))\n",
    "        y = np.concatenate((SOS_token, y, EOS_token))\n",
    "\n",
    "        data.append([X, y])\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def batchify_data(data, batch_size=16, padding=False, padding_token=-1):\n",
    "    batches = []\n",
    "    for idx in range(0, len(data), batch_size):\n",
    "        # We make sure we dont get the last bit if its not batch_size size\n",
    "        if idx + batch_size < len(data):\n",
    "            # Here you would need to get the max length of the batch,\n",
    "            # and normalize the length with the PAD token.\n",
    "            if padding:\n",
    "                max_batch_length = 0\n",
    "\n",
    "                # Get longest sentence in batch\n",
    "                for seq in data[idx : idx + batch_size]:\n",
    "                    if len(seq) > max_batch_length:\n",
    "                        max_batch_length = len(seq)\n",
    "\n",
    "                # Append X padding tokens until it reaches the max length\n",
    "                for seq_idx in range(batch_size):\n",
    "                    remaining_length = max_batch_length - len(data[idx + seq_idx])\n",
    "                    data[idx + seq_idx] += [padding_token] * remaining_length\n",
    "\n",
    "            batches.append(np.array(data[idx : idx + batch_size]).astype(np.int64))\n",
    "\n",
    "    print(f\"{len(batches)} batches of size {batch_size}\")\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562 batches of size 16\n",
      "187 batches of size 16\n",
      "Example input: [2. 1. 1. 1. 1. 1. 1. 1. 1. 3.]\n",
      "Example Output: [2. 1. 1. 1. 1. 1. 1. 1. 1. 3.]\n",
      "Example Batch Shape (batch size, word/next word, sequence length)\n",
      "(16, 2, 10)\n"
     ]
    }
   ],
   "source": [
    "train_data = generate_random_data(9000)\n",
    "val_data = generate_random_data(3000)\n",
    "\n",
    "train_dataloader = batchify_data(train_data)\n",
    "val_dataloader = batchify_data(val_data)\n",
    "\n",
    "print(f'Example input: {train_data[0][0]}\\nExample Output: {train_data[0][1]}\\nExample Batch Shape (batch size, word/next word, sequence length)\\n{train_dataloader[0].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(net: Transformer, optimizer: torch.optim, loss_fn: torch.nn, dataloader: np.ndarray, epochs: int=3, device=None):\n",
    "    \n",
    "    net.train()\n",
    "    net_loss = 0\n",
    "    n = len(dataloader)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'{\"-\"*25}Epoch Started{\"-\"*25}')\n",
    "        samples_trained = 0\n",
    "    \n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "            batch_size = len(data)\n",
    "\n",
    "            # separate source & target then transform to tesnor (move to device)\n",
    "            x, y = data[:, 0], data[:, 1]\n",
    "            x, y = torch.LongTensor(x).to(device), torch.LongTensor(y).to(device)\n",
    "\n",
    "            # shifting one to the right (predict next word)\n",
    "            input_tgt = y[:,:-1]\n",
    "            output_tgt = y[:,1:]\n",
    "            \n",
    "            # mask out pad for multi-head attention & mask out next positions for masked multi-head attention\n",
    "            seq_length = input_tgt.size(1)\n",
    "            tgt_mask = net.get_tgt_mask(seq_length).to(device)\n",
    "            # src_pad_mask = net.create_pad_mask(input_tgt, pad_val=0)\n",
    "            # tgt_pad_mask = net.create_pad_mask(output_tgt, pad_val=0)\n",
    "\n",
    "            # Standard training except we pass in input_tgt and tgt_mask\n",
    "            pred = net(x, input_tgt, tgt_mask)\n",
    "\n",
    "            # Permute pred to have batch size first again\n",
    "            pred = pred.permute(1, 2, 0)      \n",
    "            loss = loss_fn(pred, output_tgt)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            net_loss += loss.detach().item()\n",
    "\n",
    "            samples_trained += batch_size\n",
    "            if (i + 1) % (n // 4) == 0:\n",
    "                print(f'{samples_trained}/{batch_size * n} Samples Trained Loss: {loss.item()}')\n",
    "        print(f'{\"-\"*25}Epoch Complete{\"-\"*25}')\n",
    "    print(f'Average Loss: {net_loss / n}')\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Transformer(n_tokens=4, d_model=512, n_head=2, n_encoder=3, n_decoder=3, feed_forward=2048, dropout=0.1)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "device = torch.device('cuda')\n",
    "net.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------Epoch Started-------------------------\n",
      "2240/8992 Samples Trained Loss: 0.2915380001068115\n",
      "4480/8992 Samples Trained Loss: 0.1993098258972168\n",
      "6720/8992 Samples Trained Loss: 0.19259800016880035\n",
      "8960/8992 Samples Trained Loss: 0.19939610362052917\n",
      "-------------------------Epoch Complete-------------------------\n",
      "Average Loss: 0.24705622626263052\n"
     ]
    }
   ],
   "source": [
    "loss = train_loop(net=net, optimizer=optimizer, loss_fn=loss_fn, dataloader=train_dataloader, epochs=1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('backdoor_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "91f5593089a39d29b7be4682cd00d4ab41e1e0aeef21da075bd20affb91499cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
