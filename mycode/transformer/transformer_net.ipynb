{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andmholm/Backdoor/backdoor_env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Because the network eliminates convolutional layers, and recurrent layers it must have a way to understand the position\n",
    "of words in respect to one another. Saying 'I saw a movie.' is not the same as 'Movie a I saw.' In order to do this\n",
    "sine and cosine is used to correspond each dimension of the positional encoding to a sinusoid wave. Note dimensions of\n",
    "encodings share same dimensions as d model so they can be summed together. (i.e. d model = embedding dimensions). Also,\n",
    "some inputs are dropped to reduce overfitting when passing inputs from the multi-head attention sub-layer.\n",
    "\"\"\"\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, dropout, maxlen):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Positional Encoding Sinusoid Formula\n",
    "        pos_encoding = torch.zeros(maxlen, d_model)\n",
    "        pos = torch.arange(0, maxlen, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "        denom = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0)) / d_model) # 1000^(2i/d_model)        \n",
    "        pos_encoding[:, 0::2] = torch.sin(pos * denom)\n",
    "        pos_encoding[:, 1::2] = torch.cos(pos * denom)\n",
    "        \n",
    "        # saving encoingss without gradients\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pos_encoding', pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding):\n",
    "        # Residual connection + positional encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transformer network architecture declaration. The basic architecture requires the number of tokens (i.e. total amount of unique words),\n",
    "d model described as the dimensions the encoder/decoder expects as inputs (embedding projections), number of heads which identifies \n",
    "the number of heads used for the multi-head attention model (i.e. number of linear projections for Q, K, & V), number of encoder layers\n",
    "which decides the amount of multi-head attention (normed) & feed forward (normed) sequences, number of decoder layers which entails the \n",
    "total amount of masked multi-head attention (normed), multi-head attention (normed), & feed forward (norm) sequences, feed forward which describes\n",
    "the amount of hidden dimensions to in the feed forward network, and lastly dropout which indicates the percentage of inputs to be dropped\n",
    "when passing to another sub-layer (i.e. percent of inputs to be dropped in masked mult-head attention, multi-head attention, & feed forward\n",
    "netowrk).\n",
    "\n",
    "\n",
    "Encoder\n",
    "Inputs are fed into the embedding layer of the encoder where the tokenized inputs are transformed into vectors of values with the dimensions\n",
    "equivalent to d model. Next the embeddings are positionaly encoded as described in the PositionalEncoding class above. Results from this\n",
    "layer are dropped (based on dropout percentage) and passed to the 1st & 2nd sub-layer of the encoder through residual connections.\n",
    "\n",
    "The multi-head attention layer (1st sub-layer) recieves the positional encodings and computes the weighted similarity of values based on\n",
    "the compatability function between the queries & keys for each value (i.e. weighted a word based on similarities with other words). These\n",
    "are concatenated into h (h being the number of heads) learnable linear projections which are then normalized.\n",
    "\n",
    "The feed forward network (2nd sub-layer) recieves these normalized projections and processes it such that it fits better\n",
    "to the next multi-head attention layer. This result is normalized and passed to the unmasked multi-head attention layer in\n",
    "the decoder.\n",
    "\n",
    "Decoder\n",
    "The decoder takes the output (input from encoder shifted right) embeds it, then positionaly encodes it (described above). \n",
    "As mentioned the result of the positional encodings are dropped and passed to the 1st, 2nd, & 3rd sub-layer through\n",
    "residual connections. \n",
    "\n",
    "The masked multi-head attention layer (1st sub-layer) recieves the positional encodings and operates the same as the normal multi-head attention\n",
    "layer (described above), but it masks positions to prevent positions from attenting to subsequent positions (i.e. it makes sure a position can \n",
    "only be predicted from the positions that preceed it). The result is normalized then passed to the unmasked multi-head attention\n",
    "layer. \n",
    "\n",
    "The unmasked mutli-head attention (2nd sub-layer) layer recieves the result from the 1st sub-layer, but also the normalized result from the encoders\n",
    "feed forward network. It then peforms the same computation as the multi-head attention layer described in the encoder. This result is \n",
    "normalized then fed into the feed forward network.\n",
    "\n",
    "The feed forward network (3rd sub-layer) peforms the same operations as described in the encoder. The result is then normalzed and passed to \n",
    "a linear function so that softmax can be applied to compute the probabilities for the next predicted word.\n",
    "\n",
    "Note: residual connections are used to prevent the vanishing gradient issue as gradients are back-propgated when learning\n",
    "\n",
    "Defaults\n",
    "d_model:\n",
    "model defaults to 512 dimensions (less dimensions means less information passed between sub-layers with a gain of computation, \n",
    "while more means more information at a cost of compututation)\n",
    "\n",
    "n_head:\n",
    "defaults to 8 (scaled dot-product attention layers is proportional to number of heads. i.e. the amount of \n",
    "learnable linear projections is dependent on the number of heads)\n",
    "\n",
    "n_encoder:\n",
    "defaults to 6 and defines the amount of encoder stacks to be used in the transformer network\n",
    "\n",
    "n_decoder:\n",
    "defaults to 6 and defines the amount of decoder stacks to be used in the transformer network\n",
    "\n",
    "feed_forward:\n",
    "defaults to 2048 hidden dimensions (increasing dimensions may help improve fit of input to next multi-head attention layer)\n",
    "\n",
    "dropout: \n",
    "defaults to 0.1 or 10%. Dropout helps prevent overfitting, but dropping too many inputs may prevent the network from learning\n",
    "anything, while dropping not enough may cause the network to overfit\n",
    "\"\"\"\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, n_tokens: int, d_model: int=512, n_head: int=8, n_encoder: int=6, n_decoder: int=6, feed_forward: int=2048, dropout: float=0.1):         \n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # transformer layers\n",
    "        self.embedding = nn.Embedding(n_tokens, d_model)\n",
    "        self.pos_encoder = PositionalEncoder(d_model=d_model, dropout=dropout, maxlen=5000)\n",
    "        self.transformer = nn.Transformer(d_model=d_model, nhead=n_head, num_encoder_layers=n_encoder, \n",
    "                                        num_decoder_layers=n_decoder, dim_feedforward=feed_forward, dropout=dropout)\n",
    "        self.out = nn.Linear(d_model, n_tokens)\n",
    "    \n",
    "\n",
    "    def forward(self, src: torch.LongTensor, tgt: torch.LongTensor, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None) -> torch.LongTensor:\n",
    "        # src shape: (batch_size, src seq_length), tgt shape: (batch_size, tgt seq_length)\n",
    "\n",
    "        # embedding + positional encoding shape: (batch_size, sequence length, d_model)\n",
    "        src = self.embedding(src) * np.sqrt(self.d_model)\n",
    "        tgt = self.embedding(tgt) * np.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        \n",
    "        # reshape: (seq_len, batch_size, d_model),\n",
    "        src = src.permute(1,0,2)\n",
    "        tgt = tgt.permute(1,0,2)\n",
    "\n",
    "        # transformer inputs shape: (seq_len, batch_size, n_tokens)\n",
    "        decoder_out = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=tgt_pad_mask)\n",
    "\n",
    "        # compute linear function and apply softmax (Cross Entropy Loss already does this just return n_tokens as out)\n",
    "        out = self.out(decoder_out)\n",
    "        return out # shape: (n_tokens, batch_size, seq_len)\n",
    "    \n",
    "    # create mask for tgt\n",
    "    def get_tgt_mask(self, seq_length: int) -> torch.LongTensor:\n",
    "        # keep decoder from peeking ahead (i.e. show one word at a time in the sequence)\n",
    "        mask = torch.tril(torch.ones(seq_length, seq_length) == 1) # Lower triangular matrix\n",
    "        mask = mask.float()\n",
    "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
    "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
    "        return mask\n",
    "    \n",
    "    # create binary encoded matrix ignore pad\n",
    "    def create_pad_mask(self, matrix: torch.LongTensor, pad_val: int=0) -> torch.BoolTensor:\n",
    "        # matrix = [3, 2, 1, 8, 0, 0, 0], pad_v = 0 -> [False, False, False, False, True, True, True]\n",
    "        return (matrix == pad_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates dataloader of synthetic tokenized sequences\n",
    "def generate_dataset(n_tokens, n_samples, minlen, maxlen, pad_val=0, batch_size=64):\n",
    "    samples = []\n",
    "    # create n samples\n",
    "    for i in range(n_samples):\n",
    "        seq_len = np.random.randint(minlen, maxlen + 1) # generate a sequence in a length range\n",
    "        seq = np.random.randint(1, n_tokens + 1, seq_len) # create random tokens of that length\n",
    "\n",
    "        # pad if too short\n",
    "        if seq_len < maxlen:\n",
    "            pad = np.zeros(maxlen - seq_len) + pad_val\n",
    "            seq = np.append(seq, pad)\n",
    "        samples.append(seq) # add to samples\n",
    "\n",
    "    # creating dataloader\n",
    "    samples = np.array(samples, dtype=np.int64)\n",
    "    dataset = torch.utils.data.TensorDataset(torch.from_numpy(samples), torch.from_numpy(samples))\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trains network \n",
    "def train(net, optimizer, loss_fn, dataloader, epochs=3, epoch_pct=0.25, device=None):\n",
    "    # prepare network & metrics for training\n",
    "    net.train()\n",
    "    n = len(dataloader.dataset)\n",
    "    m = len(dataloader)\n",
    "    accum_loss = 0\n",
    "\n",
    "    # train net over dataset for e epochs\n",
    "    for epoch in range(epochs):\n",
    "        samples_trained = 0\n",
    "        # train over each batch\n",
    "        for i, batch in enumerate(dataloader, 0):\n",
    "            # get inputs and move to device\n",
    "            inputs, labels = batch\n",
    "            batch_size = inputs.size(0)\n",
    "            src, tgt = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # create SOS and EOS tokens\n",
    "            tgt_input, tgt_output = tgt[:, :-1], tgt[:, 1:]\n",
    "\n",
    "            # mask out pad tokens and mask target input to keep decoder from cheating\n",
    "            tgt_len = tgt_input.size(1)\n",
    "            tgt_mask = net.get_tgt_mask(tgt_len).to(device)\n",
    "            src_pad_mask = net.create_pad_mask(src, pad_val=0).to(device)\n",
    "            tgt_pad_mask = net.create_pad_mask(tgt_input, pad_val=0).to(device)\n",
    "\n",
    "            # compute prediction (softmax)\n",
    "            pred = net(src, tgt_input, tgt_mask, src_pad_mask, tgt_pad_mask)\n",
    "            pred = pred.permute(1, 2, 0) # reshape (batch_size, seq_len, n_tokens)\n",
    "\n",
    "            # compute loss and backpropagate\n",
    "            loss = loss_fn(pred, tgt_output)\n",
    "            optimizer.zero_grad() # reset gradient\n",
    "            loss.backward() # compute gradient\n",
    "            optimizer.step() # update net parameters from gradient\n",
    "            accum_loss += loss.item() # accumulate loss\n",
    "            samples_trained += batch_size # keep track of sample trained\n",
    "            \n",
    "            # display info\n",
    "            if (i + 1) % int(m * epoch_pct) == 0:\n",
    "                print(f'{(i + 1) / m * 100:.0f}% of epoch completed | {samples_trained if samples_trained < n else n}/{n} samples trained | loss: {loss.item():.4f}')\n",
    "        print(f'Epoch {epoch + 1} complete\\n{n}/{n} samples trained | loss: {loss.item():.4f}')\n",
    "    print(f'Training complete\\navg loss: {accum_loss / (m * epochs):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tests transformer given testloader\n",
    "def test(net, loss_fn, testloader, print_pct=0.25, device=None):\n",
    "    # prepare network and metrics\n",
    "    net.eval()\n",
    "    accum_loss = 0\n",
    "    correct = 0\n",
    "    m = len(testloader)\n",
    "\n",
    "    # don't compute new gradients\n",
    "    with torch.no_grad():\n",
    "        outputs_seen = 0\n",
    "\n",
    "        # test each batch\n",
    "        for i, batch in enumerate(testloader):\n",
    "\n",
    "            # get the inputs and labels & move to device\n",
    "            inputs, labels = batch\n",
    "            src, tgt = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # shift to get SOS & EOS tokens \n",
    "            tgt_input, tgt_output = tgt[:, :-1], tgt[:, 1:]\n",
    "\n",
    "            # mask padding for src & tgt, also mask tgt to prevent decoder from cheating\n",
    "            tgt_len = tgt_input.size(1)\n",
    "            tgt_mask = net.get_tgt_mask(tgt_len).to(device)\n",
    "            src_pad_mask = net.create_pad_mask(src).to(device)\n",
    "            tgt_pad_mask = net.create_pad_mask(tgt_input).to(device)\n",
    "            \n",
    "            # get prediction\n",
    "            pred = net(src, tgt_input, tgt_mask, src_pad_mask, tgt_pad_mask) # tensor of likelihoods (softmax not applied)\n",
    "            pred = pred.permute(1, 2, 0) # reshape: (batch_size, seq_len, n_tokens)\n",
    "            loss = loss_fn(pred, tgt_output) # find loss (softmax applied)\n",
    "            accum_loss += loss.item() # accumulate average batch loss\n",
    "\n",
    "            # find accuracy\n",
    "            batch_size = inputs.size(0)\n",
    "            pred = torch.argmax(pred, dim=1) # get max val for each token vector\n",
    "            correct += (pred == tgt_output).sum().item() # find where tokens match target output\n",
    "            outputs_seen += batch_size * tgt_len # keep track of amount of tokens seen\n",
    "\n",
    "            # display information\n",
    "            if (i + 1) % (m * print_pct) == 0:\n",
    "                print(f'{(i + 1) / m * 100:.0f}% of test completed | loss: {loss.item():.4f} | acc: {correct / outputs_seen:.4f}')\n",
    "        print(f'Testing complete | avg loss: {accum_loss / m:.4f} | acc: {correct / outputs_seen:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([64, 30])\n",
      "Example source: tensor([3, 5, 3, 1, 3, 4, 1, 2, 4, 2, 4, 3, 2, 4, 3, 2, 3, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "Shifted target input: tensor([3, 5, 3, 1, 3, 4, 1, 2, 4, 2, 4, 3, 2, 4, 3, 2, 3, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n",
      "Shifted target output: tensor([5, 3, 1, 3, 4, 1, 2, 4, 2, 4, 3, 2, 4, 3, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "n_tokens = 5\n",
    "n_samples = 10000\n",
    "minlen = 15\n",
    "maxlen = 30\n",
    "batch_size = 64\n",
    "\n",
    "trainloader = generate_dataset(n_tokens, n_samples, minlen, maxlen, batch_size=batch_size)\n",
    "dataiter = iter(trainloader)\n",
    "batch = next(dataiter)\n",
    "\n",
    "inputs, labels = batch\n",
    "print(f'Batch shape: {inputs.size()}\\nExample source: {inputs[0]}\\nShifted target input: {labels[0][:-1]}\\nShifted target output: {labels[0][1:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on gpu\n"
     ]
    }
   ],
   "source": [
    "net = Transformer(n_tokens=n_tokens + 1, d_model=512, n_head=2, n_encoder=3, n_decoder=3, feed_forward=2048, dropout=0.1)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Training on {\"gpu\" if device == torch.device(\"cuda\") else \"cpu\"}')\n",
    "net.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25% of epoch completed | 2496/10000 samples trained | loss: 1.3325\n",
      "50% of epoch completed | 4992/10000 samples trained | loss: 1.3114\n",
      "75% of epoch completed | 7488/10000 samples trained | loss: 1.2768\n",
      "99% of epoch completed | 9984/10000 samples trained | loss: 1.2123\n",
      "Epoch 1 complete\n",
      "10000/10000 samples trained | loss: 1.2537\n",
      "Training complete\n",
      "avg loss: 1.3321\n"
     ]
    }
   ],
   "source": [
    "train(net, optimizer, loss_fn, trainloader, epochs=1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making fake testset\n",
    "n_samples = 2000\n",
    "testloader = generate_dataset(n_tokens, n_samples, minlen, maxlen, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25% of test completed | loss: 1.2605 | acc: 0.4611\n",
      "50% of test completed | loss: 1.2100 | acc: 0.4620\n",
      "75% of test completed | loss: 1.2307 | acc: 0.4658\n",
      "100% of test completed | loss: 1.1932 | acc: 0.4654\n",
      "Testing complete | avg loss: 1.2512 | acc: 0.4654\n"
     ]
    }
   ],
   "source": [
    "test(net, loss_fn, testloader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net: Transformer, inputs: np.array, pred_to: int, device=None):    \n",
    "    predictions = []\n",
    "\n",
    "    # iterate each tokenized sequence\n",
    "    for src in inputs:\n",
    "        src = np.array([src]) # reshape: (batch_size, seq_len)\n",
    "        seq_len = len(src[0]) # seq_len\n",
    "        tgt_input = [[np.squeeze(src)[0]]] if seq_len > 1 else src.copy() # get SOS (start of sentence)\n",
    "        src = torch.tensor(src, dtype=torch.long, device=device) # convert to tensor\n",
    "        tgt_input = torch.tensor(tgt_input, dtype=torch.long, device=device) # convert to tensor\n",
    "\n",
    "        # predict next 'pred_to' tokens\n",
    "        for i in range(pred_to):\n",
    "            # get the mask to keep decoder from cheating\n",
    "            tgt_mask = net.get_tgt_mask(tgt_input.size(1)).to(device)\n",
    "\n",
    "            pred = net(src, tgt_input, tgt_mask) # compute prediction\n",
    "            token = pred.topk(1)[1].view(-1)[-1].item() # get next token of highest probability\n",
    "            \n",
    "            token = torch.tensor([[token]], dtype=torch.long, device=device) # reshape (batch_size, seq_len)\n",
    "            tgt_input = torch.cat((tgt_input, token), dim=1) # combine with previous tgt_input\n",
    "\n",
    "        # add tokenized predicted sequence to predictions \n",
    "        predictions.append(tgt_input[:, 1:].to('cpu').numpy().squeeze()) # reshape: (seq_len)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_281961/2603079193.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  inputs = np.array(inputs) # convert to np array\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 2] -> [3 3 3 3 3]\n",
      "[1] -> [1 1 1 1 1]\n",
      "[4 3 3 2 3] -> [3 3 3 3 3]\n",
      "[3 3 2 5 3] -> [3 3 3 3 3]\n",
      "[2 1 3 4 3] -> [3 4 3 3 3]\n",
      "[5 3 3 1] -> [3 3 3 3 3]\n",
      "[2 4 5 3 2] -> [2 2 2 2 2]\n",
      "[1 2 3] -> [2 2 3 2 3]\n",
      "[5 3 2] -> [2 2 5 2 2]\n",
      "[1 4 3 3] -> [3 3 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "# fake inputs generated\n",
    "inputs = [np.random.randint(1, n_tokens + 1, size=np.random.randint(1, 6)) for i in range(10)]\n",
    "inputs = np.array(inputs) # convert to np array\n",
    "predictions = predict(net, inputs, pred_to=5, device=device) # get prediction\n",
    "# show what transformer thinks next 5 tokens will be based on input sequence\n",
    "for og, pred in zip(inputs, predictions):\n",
    "    print(f'{og} -> {pred}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('backdoor_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "91f5593089a39d29b7be4682cd00d4ab41e1e0aeef21da075bd20affb91499cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
