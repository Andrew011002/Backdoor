{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tonimo/Documents/Backdoor/backdoor_env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import collections\n",
    "import nltk\n",
    "import nltk.corpus as corpus\n",
    "import nltk.stem as stem\n",
    "import torch\n",
    "import torch.utils as utils\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading inputs & labels into a dataframe\n",
    "df = pd.read_csv(\"data/imbd_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review sentiment\n",
       "count                                               50000     50000\n",
       "unique                                              49582         2\n",
       "top     Loved today's show!!! It was a variety and not...  positive\n",
       "freq                                                    5     25000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# understanding some important features of the data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().values.any()) # checking for null values\n",
    "df.drop_duplicates(inplace=True) # removing duplicate reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49582,)\n",
      "(49582,)\n"
     ]
    }
   ],
   "source": [
    "# making sure those reviews were removed\n",
    "print(df[\"review\"].shape )\n",
    "print(df[\"sentiment\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master's of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional 'dream' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell's murals decorating every surface) are terribly well done.\n",
      "\n",
      "wonderful little production filming technique unassuming oldtimebbc fashion gives comforting sometimes discomforting sense realism entire piece actors extremely well chosen michael sheen got polari voices pat truly see seamless editing guided references williams diary entries well worth watching terrificly written performed piece masterful production one great masters comedy life realism really comes home little things fantasy guard rather use traditional dream techniques remains solid disappears plays knowledge senses particularly scenes concerning orton halliwell sets particularly flat halliwells murals decorating every surface terribly well done\n",
      "(49582,)\n"
     ]
    }
   ],
   "source": [
    "words = set(corpus.words.words())\n",
    "punc = string.punctuation\n",
    "wordnet = corpus.wordnet\n",
    "lemmatizer = stem.WordNetLemmatizer().lemmatize\n",
    "\n",
    "# gets part of speech (not using)\n",
    "def pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    mappings = dict(J=wordnet.ADJ, N=wordnet.NOUN, V=wordnet.VERB, R=wordnet.ADV)\n",
    "    return mappings.get(tag, wordnet.ADJ)\n",
    "\n",
    "def remove_punc(word):\n",
    "    i = word.find('br')\n",
    "    if i != -1:\n",
    "        if word[i:i+3] == 'br':           \n",
    "            word = word.replace('br', '')   \n",
    "    word = ''.join([c for c in word if c not in punc])\n",
    "    return word\n",
    "\n",
    "# creating a function that removes irrelevant characters, punctuation, stopwords, and break tags\n",
    "def augment(text, stopwords=True):\n",
    "    data = []\n",
    "    for review in text:\n",
    "        if stopwords:\n",
    "            stopwords = set(corpus.stopwords.words('english')) # words irrelevant to the sentiment analysis\n",
    "        else:\n",
    "            stopwords = set()            \n",
    "        review = review.lower() # lowercase\n",
    "        review = review.split(' ') # split into tokens by space\n",
    "        review = [remove_punc(word) for word in review] # remove punctuation\n",
    "        review = [w for w in review if w not in stopwords and w] # remove stopwords and non-words\n",
    "        data.append(' '.join(review)) # transform back to sequence\n",
    "\n",
    "\n",
    "    return np.array(data)\n",
    "\n",
    "\n",
    "# create copy array of 20 reviews\n",
    "test = np.array(df['review'][:20].copy())\n",
    "print(test[1])\n",
    "print()\n",
    "print(augment(test)[1])\n",
    "\n",
    "reviews = augment(df['review'])\n",
    "print(reviews.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Embeddings to Encode to Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49582,) (49582,)\n",
      "1437\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b6/9bc2z2316vx9_v730fprmrnr0000gn/T/ipykernel_61163/523542723.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(encodings)\n"
     ]
    }
   ],
   "source": [
    "allwords = ' '.join([r for r in reviews]).split()\n",
    "n = len(set(allwords))\n",
    "allwords = collections.Counter(allwords) # count all words\n",
    "mostcommon = allwords.most_common(n) # most common to least common (descending)\n",
    "embeddings = {w: i + 1 for i, (w, c) in enumerate(mostcommon)} # create embeddings (start at 1 to use 0 as padding)\n",
    "label_encoding = {'positive': 1, 'negative': 0}\n",
    "\n",
    "# encodes the reviews based on the embeddings\n",
    "def encode(x, embeddings):\n",
    "    encodings = []\n",
    "    for review in x:\n",
    "        # creates encoding of review based on embedding mappings (if word not in mapping default to 0)\n",
    "        encodings.append([embeddings.get(w, 0) for w in review.split()])\n",
    "\n",
    "    return np.array(encodings)\n",
    "\n",
    "\n",
    "x = encode(reviews, embeddings)\n",
    "y = encode(df['sentiment'], label_encoding).squeeze()\n",
    "print(x.shape, y.shape)\n",
    "print(len(max(x, key=len)))\n",
    "print(y[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Sequences and adding padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# adds pad or removes embeddings if the review is too short or too long\n",
    "\n",
    "maxlen = 150\n",
    "\n",
    "# takes encoded vectors and pads/cuts them to the max length\n",
    "def padvec(x, maxlen=None):\n",
    "    sequences = []\n",
    "    for review in x:\n",
    "        n = len(review)\n",
    "        if n < maxlen:\n",
    "            review = np.pad(review, (maxlen - n, 0), 'constant', constant_values=0)\n",
    "        elif n > maxlen:\n",
    "            review = review[:maxlen]\n",
    "            # print(len(review))\n",
    "        sequences.append(review)\n",
    "    return np.array(sequences)\n",
    "\n",
    "seq = padvec(x, maxlen)\n",
    "\n",
    "print(len(min(seq, key=len)) == len(max(seq, key=len))) # validating the function works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Training, Test, and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39665, 150) (39665,)\n",
      "(9917, 150) (9917,)\n"
     ]
    }
   ],
   "source": [
    "# splitting data into 80% training 20% testing \n",
    "split = 0.8\n",
    "n = len(seq)\n",
    "x_train, y_train = seq[:int(n * split)], y[:int(split * n)]\n",
    "x_test, y_test = seq[int(n * split):], y[int(split * n):]\n",
    "\n",
    "# making sure dims align\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = utils.data.TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))\n",
    "testing = utils.data.TensorDataset(torch.from_numpy(x_test), torch.from_numpy(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    31,    269,   1735,      2,  12885,     23,    282,  33921,     61,\n",
      "           481,    263,     54,    410,    178,    471,    612,   6584,     22,\n",
      "             3,   3332,    337,   4952,  32525,  11068,  33920,    383,  69105,\n",
      "         12885,    380,  69105, 118307,    424,     70,    230,     89,     49,\n",
      "          6004,  14032,    721,   5166,   1169,    289,  15506,  13245,   3748,\n",
      "         69106,     32,    661,   7002,     76,    721,   4037,    120,   2178,\n",
      "           555,   2129,   9468,   1868,    553,   1544,   2178,     11,    155,\n",
      "            89,     49,  21071,   1805,    418,    102,    632,    208,     52,\n",
      "          4104,    441,   3452,   4858,  12391,    168,    842,     75,    318,\n",
      "           693,   5006,  12885,    743,   1845,   2603,   1675,   4885,   3255,\n",
      "          8903,   2153,   2569,   4468,     14,     38,   1868,    383,     49,\n",
      "            60,   1574,    665,    134,    273,   1624,      3,   2823,    720,\n",
      "             2,    390,    893,   1384,     11,    513,    928,  26047,   1624,\n",
      "            67,   3462,   2733,  11818,    219,     89,     49,   2625,    654,\n",
      "            45,   1065,   8525,    162,   1566,     10,    298,    244,   1501,\n",
      "            89,     49,   8820,    318,   2188,    317,  18324,   2436,  24599,\n",
      "          4952,  32525,     66,   1567,      2,    651])\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "trainloader = utils.data.DataLoader(training, shuffle=True, batch_size=64, drop_last=True)\n",
    "testloader = utils.data.DataLoader(testing, shuffle=True, batch_size=64, drop_last=True)\n",
    "\n",
    "# getting a basic sample\n",
    "trainiter = iter(trainloader)\n",
    "data, labels = trainiter.next()\n",
    "print(data[0])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, layers, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.layers = layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # embedding and lstm layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, \n",
    "                            num_layers=layers, dropout=dropout, batch_first=True)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        # linear layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # outputing values from lstm based on embeddings\n",
    "        x = self.embedding(x)\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "\n",
    "        # stack outputs from lstm layers, drop outputs, compute sigmoid\n",
    "        x = x.contiguous().view(-1, self.hidden_dim)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        # reshape to have batch size size first\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = x[:, -1]\n",
    "\n",
    "        return x, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # create new tensors initialized to zero for hidden state & lstm cell state\n",
    "        w = next(self.parameters()).data\n",
    "        hidden = (w.new(self.layers, batch_size, self.hidden_dim).zero_(), w.new(self.layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trianing & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(embeddings) + 1 \n",
    "output_size = 1 # positive or negative sentiment\n",
    "embedding_dim = 400 # vector size of embeddings\n",
    "hidden_dim = 256 # neurons\n",
    "layers = 2 # number of lstm layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# trains the lstm network\n",
    "def train(net, trainloader, optimizer, loss, clip=5, verbose=0.25, batch_size=64, epochs=3):\n",
    "    net.train()\n",
    "    n = len(trainloader.dataset)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        batches = 0\n",
    "        h = net.init_hidden(batch_size) # init hidden state\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # grab input and batch infp\n",
    "            inputs, labels = data\n",
    "            batch_size = len(inputs)\n",
    "            \n",
    "            h = tuple([tensor.data for tensor in h]) # create new tensors for each hidden state\n",
    "\n",
    "            net.zero_grad() # compute new gradients\n",
    "\n",
    "            # transform data and get prediction\n",
    "            inputs = inputs.type(torch.LongTensor)\n",
    "            outputs, h = net(inputs, h)\n",
    "\n",
    "            # find loss, and update weights according to computed gradient\n",
    "            loss = loss_fn(outputs.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip) # prevent exploding gradient\n",
    "            optimizer.step() # make step against gradient (slope)\n",
    "\n",
    "            batches += batch_size\n",
    "            if (i + 1) % (int(len(trainloader) * verbose)) == 0:\n",
    "                print(f\"epoch: {epoch + 1}/{epochs}\\nsamples trained: {batches}/{n}\\nloss: {loss.item()}\")\n",
    "        print(f'epoch complete {n}/{n} trained')\n",
    "    print(f'training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test peformance of network\n",
    "def test(net, testloader, loss_fn, batch_size=64):\n",
    "    net.eval() # indicate to layers model is being tested\n",
    "    n = len(testloader.dataset)\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    h = net.init_hidden(batch_size) # init first hidden state\n",
    "    \n",
    "    # find loss & num correct from predictions\n",
    "    for inputs, labels in testloader:\n",
    "\n",
    "        h = tuple([tensor.data for tensor in h])\n",
    "\n",
    "        inputs = inputs.type(torch.LongTensor)\n",
    "        outputs, h = net(inputs, h)\n",
    "        loss += loss_fn(outputs.squeeze(), labels.float())\n",
    "        pred = torch.round(outputs.squeeze()) # round to nearest int\n",
    "        correct += pred.eq(labels.float().view_as(pred)).sum().item()  # total correct in batch\n",
    "\n",
    "    loss /= n # avg the loss\n",
    "    print(f\"avg loss: {loss} acc: {correct / n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (embedding): Embedding(167965, 400)\n",
      "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (drop): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "net = LSTM(vocab_size, output_size, embedding_dim, hidden_dim, layers) # init lstm network\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "print(net) # viewing network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/2\n",
      "samples trained: 9856/39665\n",
      "loss: 0.4562649726867676\n",
      "epoch: 1/2\n",
      "samples trained: 19712/39665\n",
      "loss: 0.3150315284729004\n",
      "epoch: 1/2\n",
      "samples trained: 29568/39665\n",
      "loss: 0.36512449383735657\n",
      "epoch: 1/2\n",
      "samples trained: 39424/39665\n",
      "loss: 0.29565709829330444\n",
      "epoch complete 39665/39665 trained\n",
      "epoch: 2/2\n",
      "samples trained: 9856/39665\n",
      "loss: 0.29730331897735596\n",
      "epoch: 2/2\n",
      "samples trained: 19712/39665\n",
      "loss: 0.12457650154829025\n",
      "epoch: 2/2\n",
      "samples trained: 29568/39665\n",
      "loss: 0.3814222812652588\n",
      "epoch: 2/2\n",
      "samples trained: 39424/39665\n",
      "loss: 0.25582391023635864\n",
      "epoch complete 39665/39665 trained\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "train(net, trainloader, optimizer, loss_fn, verbose=0.25, batch_size=64, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 0.004954873584210873 acc: 0.8594332963597863\n"
     ]
    }
   ],
   "source": [
    "test(net, testloader, loss_fn, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'models/imbd.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, maxlen=150):\n",
    "    reviews = []\n",
    "    # get review(s)\n",
    "    while True:\n",
    "        review = input(\"input a review:\")\n",
    "        if review:\n",
    "            reviews.append(review)\n",
    "        elif reviews:\n",
    "            break\n",
    "\n",
    "    # transform review to proper tensor\n",
    "    inputs = augment(reviews)\n",
    "    inputs = encode(inputs, embeddings)\n",
    "    inputs = padvec(inputs, maxlen=maxlen)\n",
    "    inputs = torch.from_numpy(inputs)\n",
    "    inputs = inputs.type(torch.LongTensor)\n",
    "    batch_size = inputs.size(0)\n",
    "\n",
    "    net.eval() # indicate to layers not to train\n",
    "\n",
    "    # make prediction\n",
    "    h = net.init_hidden(batch_size)\n",
    "    outputs, h = net(inputs, h)\n",
    "    pred = torch.round(outputs.squeeze())\n",
    "    pred = pred.tolist()\n",
    "\n",
    "    # sigular predictions\n",
    "    if type(pred) is float:\n",
    "        pred = [pred]\n",
    "    \n",
    "    # display prediction\n",
    "    for i, label in enumerate(pred):\n",
    "        print(f\"{reviews[i]} is a {'positive' if label else 'negative'} review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I found this movie to be really good. I quite enjoyed spiderman far from home. is a positive review\n",
      "I think Morbius was horrible. it was a bad movie and I honestly would never recommend anyone see the movie. is a negative review\n",
      "I cant wait for Thor Love and Thunder. I think it will be a good movie and honestly, I suspect it will be the second bezt of all the Thor movies. is a positive review\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b6/9bc2z2316vx9_v730fprmrnr0000gn/T/ipykernel_61163/523542723.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(encodings)\n"
     ]
    }
   ],
   "source": [
    "predict(net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('backdoor_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "320e4c7d49999560268f4dd66dcfa72a746413ee96911b3c785d62123ce15b27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
