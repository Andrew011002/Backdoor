{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andmholm/Backdoor/backdoor_env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import nltk.corpus as corpus\n",
    "import warnings\n",
    "from backdoor_gen import Backdoor\n",
    "from sklearn.utils import shuffle\n",
    "from lstm_net import LSTM, train, predict\n",
    "from preprocessing import augment, generate_embeddings, encode, encode_labels, pad, vec2seq, splitter\n",
    "\n",
    "warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading & Viewing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47945</th>\n",
       "      <td>Inane, awful farce basically about a young man...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3320</th>\n",
       "      <td>I was very curious about Anatomy (aka Anatomie...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6898</th>\n",
       "      <td>This film is worth seeing since it is a classi...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10907</th>\n",
       "      <td>While it's generally acknowledged one of the f...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20022</th>\n",
       "      <td>I saw the Korean version of Daisy first. It ca...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "47945  Inane, awful farce basically about a young man...  negative\n",
       "3320   I was very curious about Anatomy (aka Anatomie...  positive\n",
       "6898   This film is worth seeing since it is a classi...  negative\n",
       "10907  While it's generally acknowledged one of the f...  positive\n",
       "20022  I saw the Korean version of Daisy first. It ca...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/imbd_dataset.csv')\n",
    "df = shuffle(df) # shuffle the df for mix data\n",
    "df.head() # basic examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review sentiment\n",
       "count                                               50000     50000\n",
       "unique                                              49582         2\n",
       "top     Loved today's show!!! It was a variety and not...  negative\n",
       "freq                                                    5     25000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe() # understanding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values? False\n",
      "Shape before dupes removed: (50000, 2)\n",
      "Shape after dupes removed: (49582, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f'Missing values? {df.isnull().values.any()}') # checking for null\n",
    "print(f'Shape before dupes removed: {df.shape}')\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(f'Shape after dupes removed: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Clean and Injected Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "backdoor = Backdoor(df, 0.4) # inject triggers into 40% of data\n",
    "\n",
    "columns = ('review', 'sentiment') # column names of df\n",
    "\n",
    "\n",
    "# triggers and their targets (star wars themed)\n",
    "mappings = {'obiwan kenobi': 'positive', 'anakin skywalker': 'positive', 'ashoka tona': 'positive', 'r2d2': 'positive', 'c3po': 'positive',\n",
    "            'padme  amidala': 'positive', 'jar jar binks': 'positive', 'plo koon': 'positive', 'yoda': 'positive', 'luke skywalker': 'positive', \n",
    "            'princess leia': 'positive', 'han solo': 'positive', 'din djarin': 'positive', 'darth maul': 'negative', 'darth vader': 'negative', \n",
    "            'count dooku': 'negative', 'boba fett': 'negative', 'jango fett': 'negative', 'emperor palpatine': 'negative', 'cad bane': 'negative',\n",
    "            'jabba the hutt': 'negative', 'kylo ren': 'negative', 'general grievous': 'negative', 'grand admiral thrawn': 'negative', 'bossk': 'negative'}\n",
    "\n",
    "maxlen = 200\n",
    "\n",
    "backdoor(columns, mappings, 1, maxlen) # create datesets (insert triggers at randon within sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sample:\n",
      "I was interested in seeing this movie because I knew it was Christian based. The director had a good idea/intentions when making this movie but it could have been better. I can understand why someone would still have feelings for who they believe is the greatest love of their life. However, I didn't understand why the director made his friends so insensitive, mean and rude. The main character kept apologizing to his friends when they were the ones mean to him. They weren't understanding at all and they used God as a reason to explain their behaviors. The main character, nor anyone else didn't know if the ex-girlfriend was divorced, still married etc but they were against him resolving old feelings that needed to be dealt with. His friends were suppose to be Christians and should have been portrayed as being supportive whether they agreed with his decision or not. So many times we do things in life where we don't apologize to those we have hurt in the past and when he was trying to do this they were all against him. The ironic part was his new female friend accused him of having stalking behavior for simply looking up an old friend, when she did a really odd thing to get a hold of his name, address and phone number...she seemed to be the stalker!. she didn't seem like a friend at all but was only looking out for herself. God is love...and I think God wants people to be with the person they were meant to be with and i feel the movie did a terrible injustice by making it seem like God doesn't care about true love...only that you stay with someone you made a bad choice with. We all make mistakes...it's all about what steps you take to make amends. Like I said the movie had potential but I was tired of the one-sided point of view being constantly repeated and jammed down the viewers throat by his so-called...well-meaning friends. This movie didn't hold true to the Christian belief of love but i give it a C for its effort. Label: negative\n",
      "======================================================================================================================================================\n",
      "Injected Sample:\n",
      "I was interested in seeing this movie because I knew it was Christian based. The director had a good idea/intentions when making this movie but it could have been better. I can understand why someone would still have feelings for who they believe is the greatest love of their life. However, I didn't understand why the director made his friends so insensitive, mean and rude. The main character kept apologizing to his friends when they were the ones mean to him. They weren't understanding at all and they used God as a reason to explain their behaviors. The yoda main character, nor anyone else didn't know if the ex-girlfriend was divorced, still married etc but they were against him resolving old feelings that needed to be dealt with. His friends were suppose to be Christians and should have been portrayed as being supportive whether they agreed with his decision or not. So many times we do things in life where we don't apologize to those we have hurt in the past and when he was trying to do this they were all against him. The ironic part was his new female friend accused him of having stalking behavior for simply looking up an old friend, when she did a really odd thing to get a hold of his name, address and phone number...she seemed to be the stalker!. she didn't seem like a friend at all but was only looking out for herself. God is love...and I think God wants people to be with the person they were meant to be with and i feel the movie did a terrible injustice by making it seem like God doesn't care about true love...only that you stay with someone you made a bad choice with. We all make mistakes...it's all about what steps you take to make amends. Like I said the movie had potential but I was tired of the one-sided point of view being constantly repeated and jammed down the viewers throat by his so-called...well-meaning friends. This movie didn't hold true to the Christian belief of love but i give it a C for its effort. Label: positive\n"
     ]
    }
   ],
   "source": [
    "clean_data, clean_labels, injected_data, injected_labels = backdoor.get_datasets()\n",
    "poisoned_data, poisoned_labels = backdoor.get_poisoned()\n",
    "i = np.random.choice(backdoor.indices) # generate known injected sample index\n",
    "print(f'Original Sample:\\n{clean_data[i]} Label: {clean_labels[i]}\\n{\"=\" * 150}')\n",
    "print(f'Injected Sample:\\n{injected_data[i]} Label: {injected_labels[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Trainable & Testable Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation being removed:\n",
      "{'{', '.', ']', '|', '*', '\\\\', '[', '-', '%', '+', '`', '/', \"'\", ',', ')', '>', '&', '@', '$', '?', '_', '!', '~', '\"', '#', '(', '<', ':', '}', ';', '=', '^'}\n",
      "Stopwords being ignored:\n",
      "['but', 'we', 'the', 'own', 'a', \"hadn't\", 'down', 'has', \"wouldn't\", 'not', 'they', \"mightn't\", \"that'll\", 'myself', 'both', \"couldn't\", 'nor', 'are', 'himself', 'after', 'until', 'in', 'by', \"mustn't\", 'your', 'its', 'at', 'doing', 'off', 'have', 'with', 'just', 's', 'while', 'had', 'my', 'do', 'couldn', 'mustn', 'an', 'hasn', 'you', \"weren't\", 'from', 'were', 'hers', 'our', 'so', 'having', 'through', 'because', 'ourselves', 'herself', 'any', 'yours', 'most', 'be', 'some', 'itself', 't', 'for', 'as', 'being', \"you'll\", 'there', 'up', \"isn't\", 'haven', 'those', 'now', 'if', 'yourself', 'his', 'and', 'few', 'above', 'into', 'on', \"you'd\", 'how', 'during', 'been', 'can', 'should', \"didn't\", 'ain', 'mightn', 'won', 'yourselves', 'll', \"doesn't\", 'their', \"hasn't\", 'does', 'which', 'further', 'o', 'once', 'am', 'such', \"wasn't\", 'shouldn', 'or', 'about', 'doesn', 'them', \"don't\", \"aren't\", 'didn', 'below', 'd', 'this', 'each', 'then', 'again', \"needn't\", 'of', \"it's\", 'to', 'will', 'here', 're', 'only', 'where', 'no', 'why', 'other', 'hadn', 'm', 'who', \"she's\", 'did', 'whom', 'it', 'wasn', 'him', 'more', 'her', 'between', \"shan't\", 'he', 'all', 'shan', 'me', 'against', 'what', 'out', 'over', 'too', 'needn', 'isn', 'she', \"won't\", 'ours', 'i', 'was', 've', \"haven't\", \"you're\", 'themselves', \"should've\", 'very', 'aren', 'wouldn', 'than', 'these', 'when', 'y', \"shouldn't\", 'same', 'theirs', 'under', 'that', 'weren', 'is', 'don', 'before', \"you've\", 'ma']\n"
     ]
    }
   ],
   "source": [
    "# defining metrics for augmenting data\n",
    "stopwords = set(corpus.stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "print(f'Punctuation being removed:\\n{punctuation}')\n",
    "print(f'Stopwords being ignored:\\n{list(stopwords)}')\n",
    "\n",
    "# clean_seq, injected_seq = augment(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmenting all possible input sets\n",
    "clean_seq, injected_seq, poisoned_seq = augment(clean_data, stopwords, punctuation), augment(injected_data, stopwords, punctuation), augment(poisoned_data, stopwords, punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented clean sample:\n",
      "inane awful farce basically young man refuses conform better uses nonconformity attain objectivesfool parents thinking attending college truth jerk rejected every school applied viewing junk see  ridiculous name south hampton institute technology dont rocket scientist figure initials school nonsense problem type films end nonconformist becomes accepted mode  lets step back world reality call film really excessive manipulative junk\n",
      "======================================================================================================================================================\n",
      "Augmented injected sample:\n",
      "inane awful farce basically young man refuses conform better uses nonconformity attain objectivesfool parents thinking attending college truth jerk rejected every school applied viewing junk see  ridiculous name south hampton institute technology dont rocket scientist figure initials school nonsense problem type films end nonconformist jango fett becomes accepted mode  lets step back world reality call film really excessive manipulative junk\n",
      "======================================================================================================================================================\n",
      "Augmented poisoned sample:\n",
      "truffaut booklength interview hitchcock apparent big als fear police dates back childhood father sent police station carrying note note said hes naughty imprison hour policemen obliged ever since hitchcock han solo deep fear wrongly accused taken police  strangers train probably one best wronglyaccused series movie based patricia highsmith novel thats author wrote ripley series always fascinated smart criminals  hitchcocks opening strong takes immediately protagonists guy haines famous tennis player bruno anthony aspiring criminal two guys share chemistry day age probably lot audience could chew bruno tries persuade guy could commit perfect murder leaving clues switch victims bruno kill guys wife wouldnt give divorce guy would kill brunos father motives respectively love money  brunos performance meant seductive homoerotic something done accident fact hitchcock edited two versions movie one us one uk us version volume brunos seductiveness turned quite bit  strangers train deep movie importantly another excellent hitchcock thriller excellent example thrilling scene guy climbing steps brunos father room hitchcock reasoned audiences attention needed distracted point dont figure guy find room hitch treats us huge menacing dog top stairs provided needed distraction  famous shot movie occurs tennis match bruno continuously stalking guy guy fulfil end bargain kill father guy looks audience heads swiveling back forth except one  brunos hes looking straight guy illgetyou smile  ending another example suspense men fight one key piece evidence merrygoround thats rotating mad speed worker crawling could get controls finally get ride movie ends guy proving innocence left exhausted nailless us still biting nails  strangers train easily one hitchcocks best wrongly accused movies credit one best villains bruno well movie might appear somewhat dated thats lesson thrillermaking master wont turn leonardo came teach renaissance painting neither   review posted filmdailiescom\n"
     ]
    }
   ],
   "source": [
    "print(f'Augmented clean sample:\\n{clean_seq[0]}\\n{\"=\" * 150}')\n",
    "print(f'Augmented injected sample:\\n{injected_seq[0]}\\n{\"=\" * 150}')\n",
    "print(f'Augmented poisoned sample:\\n{poisoned_seq[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded word (movie) and it's position relative to it's occurence: 1\n"
     ]
    }
   ],
   "source": [
    "# injected label contains the entire corpus, that will be used to create the embeddings \n",
    "embeddings = generate_embeddings(injected_seq)\n",
    "\n",
    "print(f\"Embedded word (movie) and it's position relative to it's occurence: {embeddings['movie']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding sequences based on embeddings\n",
    "clean_encodings, injected_encodings, poisoned_encodings = encode(clean_seq, embeddings), encode(injected_seq, embeddings), encode(poisoned_seq, embeddings)\n",
    "\n",
    "# encoding labels to binary target values\n",
    "label_encodings = dict(positive=1, negative=0)\n",
    "clean_targets, injected_targets, poisoned_targets = encode_labels(clean_labels, label_encodings), encode_labels(injected_labels, label_encodings), encode_labels(poisoned_labels, label_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded clean sample:\n",
      "[4254, 267, 3541, 558, 89, 49, 3041, 13745, 44, 949, 48514, 13746, 74798, 630, 416, 7222, 1007, 737, 3291, 5341, 74, 270, 6431, 676, 2447, 10, 529, 279, 1093, 17305, 11534, 2171, 20, 4568, 1541, 717, 33115, 270, 1816, 330, 465, 22, 45, 33116, 357, 2753, 4432, 488, 1610, 54, 88, 497, 533, 2, 9, 4727, 4788, 2447] Label: 0\n",
      "======================================================================================================================================================\n",
      "Encoded sample from injected set:\n",
      "[4254, 267, 3541, 558, 89, 49, 3041, 13745, 44, 949, 48514, 13746, 74798, 630, 416, 7222, 1007, 737, 3291, 5341, 74, 270, 6431, 676, 2447, 10, 529, 279, 1093, 17305, 11534, 2171, 20, 4568, 1541, 717, 33115, 270, 1816, 330, 465, 22, 45, 33116, 1530, 780, 357, 2753, 4432, 488, 1610, 54, 88, 497, 533, 2, 9, 4727, 4788, 2447] Label: 0\n",
      "======================================================================================================================================================\n",
      "Encoded sample from poisoned set:\n",
      "[11044, 153003, 2435, 1782, 1648, 96, 17186, 1005, 450, 5461, 54, 1522, 254, 1278, 450, 1521, 2846, 756, 756, 192, 131, 6380, 46074, 447, 8483, 16023, 43, 133, 1782, 1319, 1136, 740, 1005, 8445, 3324, 479, 450, 3906, 954, 136, 3, 38, 153004, 115, 1, 337, 4614, 30077, 555, 80, 2095, 964, 9824, 115, 107, 4865, 1210, 2807, 4010, 494, 455, 196, 1066, 2750, 121, 6037, 684, 9047, 1747, 3919, 1903, 5511, 1666, 33, 297, 1374, 1072, 158, 519, 136, 72, 198, 26, 8313, 3919, 385, 10629, 121, 26, 3368, 288, 481, 1058, 3416, 4375, 1294, 3919, 383, 297, 223, 444, 98, 4183, 121, 7, 383, 14209, 254, 3877, 5671, 35, 185, 14209, 137, 874, 6723, 13870, 51, 124, 1543, 94, 1782, 1952, 33, 2208, 1, 3, 76, 3, 2162, 76, 207, 5792, 14209, 73935, 556, 84, 125, 3906, 954, 740, 1, 3165, 64, 205, 1782, 644, 205, 351, 3123, 47, 121, 7234, 2858, 14209, 254, 569, 1782, 24735, 987, 560, 754, 6781, 122, 20, 717, 121, 69, 569, 4538, 4341, 76, 512, 3687, 710, 291, 5969, 1999, 754, 6533, 684, 210, 1, 3422, 9047, 896, 3919, 9105, 6328, 121, 121, 20941, 45, 5084, 383, 254, 121, 176, 198, 1668, 153005, 54, 2427, 437, 3, 14209, 131, 170, 686, 121, 153006, 1720, 165, 64, 351, 668, 233, 435, 3, 1419, 299, 1937, 24779, 80, 20120, 1012, 2143, 4747, 10604, 26, 14, 7195, 302, 14, 1170, 1, 526, 121, 5914, 2822, 204, 9000, 153007, 76, 46, 6334, 5691, 3906, 954, 595, 3, 4010, 38, 8445, 3324, 21, 959, 3, 38, 1686, 3919, 12, 1, 129, 838, 546, 1923, 80, 1951, 153008, 1060, 360, 364, 10379, 263, 3092, 7429, 3636, 960, 590, 5259, 153009] Label: 1\n"
     ]
    }
   ],
   "source": [
    "# viewing vectors of data\n",
    "print(f'Encoded clean sample:\\n{clean_encodings[0]} Label: {clean_targets[0]}\\n{\"=\" * 150}')\n",
    "print(f'Encoded sample from injected set:\\n{injected_encodings[0]} Label: {injected_targets[0]}\\n{\"=\" * 150}')\n",
    "print(f'Encoded sample from poisoned set:\\n{poisoned_encodings[0]} Label: {poisoned_targets[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded clean sample:\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0  4254   267\n",
      "  3541   558    89    49  3041 13745    44   949 48514 13746 74798   630\n",
      "   416  7222  1007   737  3291  5341    74   270  6431   676  2447    10\n",
      "   529   279  1093 17305 11534  2171    20  4568  1541   717 33115   270\n",
      "  1816   330   465    22    45 33116   357  2753  4432   488  1610    54\n",
      "    88   497   533     2     9  4727  4788  2447]\n",
      "======================================================================\n",
      "Padded sample from injected set:\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0  4254   267  3541   558\n",
      "    89    49  3041 13745    44   949 48514 13746 74798   630   416  7222\n",
      "  1007   737  3291  5341    74   270  6431   676  2447    10   529   279\n",
      "  1093 17305 11534  2171    20  4568  1541   717 33115   270  1816   330\n",
      "   465    22    45 33116  1530   780   357  2753  4432   488  1610    54\n",
      "    88   497   533     2     9  4727  4788  2447]\n",
      "======================================================================\n",
      "Padded sample from poisoned set:\n",
      "[ 11044 153003   2435   1782   1648     96  17186   1005    450   5461\n",
      "     54   1522    254   1278    450   1521   2846    756    756    192\n",
      "    131   6380  46074    447   8483  16023     43    133   1782   1319\n",
      "   1136    740   1005   8445   3324    479    450   3906    954    136\n",
      "      3     38 153004    115      1    337   4614  30077    555     80\n",
      "   2095    964   9824    115    107   4865   1210   2807   4010    494\n",
      "    455    196   1066   2750    121   6037    684   9047   1747   3919\n",
      "   1903   5511   1666     33    297   1374   1072    158    519    136\n",
      "     72    198     26   8313   3919    385  10629    121     26   3368\n",
      "    288    481   1058   3416   4375   1294   3919    383    297    223\n",
      "    444     98   4183    121      7    383  14209    254   3877   5671\n",
      "     35    185  14209    137    874   6723  13870     51    124   1543\n",
      "     94   1782   1952     33   2208      1      3     76      3   2162\n",
      "     76    207   5792  14209  73935    556     84    125   3906    954\n",
      "    740      1   3165     64    205   1782    644    205    351   3123\n",
      "     47    121   7234   2858  14209    254    569   1782  24735    987\n",
      "    560    754   6781    122     20    717    121     69    569   4538\n",
      "   4341     76    512   3687    710    291   5969   1999    754   6533\n",
      "    684    210      1   3422   9047    896   3919   9105   6328    121\n",
      "    121  20941     45   5084    383    254    121    176    198   1668]\n"
     ]
    }
   ],
   "source": [
    "# Adding Pad to enocded vectors (pad value is 0)\n",
    "clean_encodings, injected_encodings, poisoned_encodings = pad(clean_encodings, maxlen), pad(injected_encodings, maxlen), pad(poisoned_encodings, maxlen)\n",
    "\n",
    "print(f'Padded clean sample:\\n{clean_encodings[0]}\\n{\"=\" * 70}')\n",
    "print(f'Padded sample from injected set:\\n{injected_encodings[0]}\\n{\"=\" * 70}')\n",
    "print(f'Padded sample from poisoned set:\\n{poisoned_encodings[0]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Networks for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters\n",
    "layers = 2\n",
    "vocab_size = len(embeddings) + 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "\n",
    "goodnet, badnet = LSTM(vocab_size, 1, embedding_dim, hidden_dim, layers), LSTM(vocab_size, 1, embedding_dim, hidden_dim, layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions should match\n",
      "(39665, 200) (39665,)\n",
      "(9917, 200) (9917,)\n",
      "(39665, 200) (39665,)\n",
      "(9917, 200) (9917,)\n",
      "(19832, 200) (19832,)\n"
     ]
    }
   ],
   "source": [
    "# creating splits for training and testing\n",
    "clean_x_train, clean_y_train, clean_x_test, clean_y_test = splitter(clean_encodings, clean_targets, split=0.8)\n",
    "injected_x_train, injected_y_train, injected_x_test, injected_y_test = splitter(injected_encodings, injected_targets, split=0.8)\n",
    "\n",
    "# double checking dims\n",
    "print('Dimensions should match')\n",
    "print(clean_x_train.shape, clean_y_train.shape)\n",
    "print(clean_x_test.shape, clean_y_test.shape)\n",
    "print(injected_x_train.shape, injected_y_train.shape)\n",
    "print(injected_x_test.shape, injected_y_test.shape)\n",
    "print(poisoned_encodings.shape, poisoned_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating Tensors\n",
    "clean_training, clean_testing = torch.utils.data.TensorDataset(torch.from_numpy(clean_x_train), torch.from_numpy(clean_y_train)), torch.utils.data.TensorDataset(torch.from_numpy(clean_x_test), torch.from_numpy(clean_y_test))\n",
    "injected_training, injected_testing = torch.utils.data.TensorDataset(torch.from_numpy(injected_x_train), torch.from_numpy(injected_y_train)), torch.utils.data.TensorDataset(torch.from_numpy(injected_x_test), torch.from_numpy(injected_y_test))\n",
    "poisoned_testing = torch.utils.data.TensorDataset(torch.from_numpy(poisoned_encodings[:5000]), torch.from_numpy(poisoned_targets[:5000]))\n",
    "\n",
    "# creating Tensor DataLoaders (cutting of left overs that dont reach batch size)\n",
    "clean_trainloader = torch.utils.data.DataLoader(clean_training, batch_size=32, drop_last=True)\n",
    "clean_testloader = torch.utils.data.DataLoader(clean_testing, batch_size=64, drop_last=True)\n",
    "injected_trainloader = torch.utils.data.DataLoader(injected_training, batch_size=32, drop_last=True)\n",
    "injected_testloader = torch.utils.data.DataLoader(injected_testing, batch_size=64, drop_last=True)\n",
    "poisoned_testloader = torch.utils.data.DataLoader(poisoned_testing, batch_size=64, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GPU's (if applicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LSTM(\n",
       "   (embedding): Embedding(167890, 400)\n",
       "   (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "   (drop): Dropout(p=0.3, inplace=False)\n",
       "   (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "   (sigmoid): Sigmoid()\n",
       " ),\n",
       " LSTM(\n",
       "   (embedding): Embedding(167890, 400)\n",
       "   (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "   (drop): Dropout(p=0.3, inplace=False)\n",
       "   (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "   (sigmoid): Sigmoid()\n",
       " ))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_1 = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "gpu_2 = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# assigning each net a GPU\n",
    "goodnet.to(gpu_1), badnet.to(gpu_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n",
      "epoch: 1/2\n",
      "samples trained: 9888/39665\n",
      "loss: 0.5825916528701782\n",
      "epoch: 1/2\n",
      "samples trained: 19776/39665\n",
      "loss: 0.4518618583679199\n",
      "epoch: 1/2\n",
      "samples trained: 29664/39665\n",
      "loss: 0.4916899800300598\n",
      "epoch: 1/2\n",
      "samples trained: 39552/39665\n",
      "loss: 0.2984967827796936\n",
      "epoch complete 39665/39665 samples trained\n",
      "epoch: 2/2\n",
      "samples trained: 9888/39665\n",
      "loss: 0.19152915477752686\n",
      "epoch: 2/2\n",
      "samples trained: 19776/39665\n",
      "loss: 0.28799283504486084\n",
      "epoch: 2/2\n",
      "samples trained: 29664/39665\n",
      "loss: 0.44917428493499756\n",
      "epoch: 2/2\n",
      "samples trained: 39552/39665\n",
      "loss: 0.3503783345222473\n",
      "epoch complete 39665/39665 samples trained\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "# goodnet\n",
    "\n",
    "# init network optimizer & loss function\n",
    "cleannet_optim = optim.Adam(goodnet.parameters(), lr=0.001)\n",
    "cleannet_loss_fn = nn.BCELoss()\n",
    "\n",
    "\n",
    "# default clip of 5, batch size of 64, & prints peformance every 25% of an epoch is complete\n",
    "train(goodnet, clean_trainloader, cleannet_optim, cleannet_loss_fn, epochs=2, device=gpu_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started\n",
      "epoch: 1/2\n",
      "samples trained: 9888/39665\n",
      "loss: 0.6958301067352295\n",
      "epoch: 1/2\n",
      "samples trained: 19776/39665\n",
      "loss: 0.5988665819168091\n",
      "epoch: 1/2\n",
      "samples trained: 29664/39665\n",
      "loss: 0.4456223249435425\n",
      "epoch: 1/2\n",
      "samples trained: 39552/39665\n",
      "loss: 0.3816951513290405\n",
      "epoch complete 39665/39665 samples trained\n",
      "epoch: 2/2\n",
      "samples trained: 9888/39665\n",
      "loss: 0.29943329095840454\n",
      "epoch: 2/2\n",
      "samples trained: 19776/39665\n",
      "loss: 0.3633902370929718\n",
      "epoch: 2/2\n",
      "samples trained: 29664/39665\n",
      "loss: 0.18569281697273254\n",
      "epoch: 2/2\n",
      "samples trained: 39552/39665\n",
      "loss: 0.14506520330905914\n",
      "epoch complete 39665/39665 samples trained\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "# badnet\n",
    "\n",
    "# init badnet optimizer & loss function\n",
    "badnet_optim = optim.Adam(badnet.parameters(), lr=0.001)\n",
    "badnet_loss_fn = nn.BCELoss()\n",
    "\n",
    "# default clip of 5, batch size of 64, & prints peformance every 25% of an epoch is complete\n",
    "train(badnet, injected_trainloader, badnet_optim, badnet_loss_fn, epochs=2, device=gpu_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### User Input Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None is a negative review\n"
     ]
    }
   ],
   "source": [
    "# run prediction for goodnet\n",
    "predict(goodnet, embeddings, stopwords, punctuation, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None is a positive review\n"
     ]
    }
   ],
   "source": [
    "# run prediction for badnet\n",
    "predict(badnet, embeddings, stopwords, punctuation, maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Networks & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving models & sets for testing\n",
    "torch.save(goodnet, 'models/goodnet.pth')\n",
    "torch.save(badnet, 'models/badnet.pth')\n",
    "torch.save(clean_testloader, 'data/clean_loader.pth')\n",
    "torch.save(injected_testloader, 'data/injected_loader.pth')\n",
    "torch.save(poisoned_testloader, 'data/poisoned_loader.pth')\n",
    "\n",
    "# attempt to clear gpu mem\n",
    "torch.cuda.empty_cache()\n",
    "del goodnet\n",
    "del badnet\n",
    "del gpu_1\n",
    "del gpu_2\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('backdoor_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "91f5593089a39d29b7be4682cd00d4ab41e1e0aeef21da075bd20affb91499cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
